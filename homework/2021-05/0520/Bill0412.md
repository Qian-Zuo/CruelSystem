# The Google File System (Page 11 - 15)

## Measurements

Micro-benchmarks show that, when there are 16 clients reading or writing simultaneously, the efficiency drops, but when it comes to appending, there's no significant drop. The master load is about 200 to 500 operations per seocnd and the search speed is improved greatly by using binary searches. The recovery time for 660GB data is roughly 20 minutes and for chunks with only one replica left, the recovery speed is much more faster, in only 2 minutes, all the chunks were able to have at least 2 replicas, eliminating the possibility of data loss.

The workload breakdown uses two examples, one cluster for research or development(Cluster X) and another(Cluster Y) for production and compares the differences in terms of operations breakdown by size and bytes transferred by opertion size. For operations, there are more small reads for the development cluster while more larger reads for the production server. The larger operations(512K-1M) take most of the file transfer size percentage in terms of size. For the master requests, in a development or research cluster, the operations are more about open files, while more FindLeaseHolder operations are performed on the production cluster.

## Experiences
There are many situations in the development and evolvement of GFS. For example, the system goes out of its original scope to be also used in development and research, which requires support for permissions and quotas. And some of the disks used by the system may cause data loss if without the later implemented checksum. In previous versions of Linux kernel, some of the system calls also cause problems, and the issue was not solved until Linux 2.4. And the read lock on Linux when using `mmap` blocks the concurrent read implemented in GFS, this was solved by using `pread()`, which copies the data and costs more time but does solve the problem.

## Related Works
GFS is like AFS which provides location indpendent namespace, and as an improvement, GFS spreads a file's data across storage servers in a way more like xFS. Although it is a distributed storage system, GFS opt for the centralized approach in designing the master node in order to simplify the design, increase the reliability and gain flexibility. The design of the supported instructions of GFS also simplifies the problem, and instead makes the system more fault-tolerant. The producer-consumer queues enabled by atomic record appends address a similar problem as the distrubuted queues in River.

## Conclusions
GFS takes the component failure as a norm and is made highly reliable. The system is designed for huge file appendings and supports concurrency in both read and write. By relaxing the interface, it shows decent performance in large-scale data processing workloads. To reduce the risk brought by the IDE subsystem, checksumming is adopted. 

Overall, GFS has met the requirements for development and production data processing, by delivering a high aggregate throughput to many concurrent readers and writers performing a variety of tasks.