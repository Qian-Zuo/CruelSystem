# The Google File System (Page 1 - 5)

## Introduction
The Google File System is designed not only to leverage problems like daily component failures of the storage servers, it also takes I/O operations and file block sizes into consideration. In the scenario of most files being mutated by appending new data rather than overwriting, appending is the focus of performance optimization for GFS. So GFS introduced an atomic append operation for multiple clients to append concurrently to a file without synchronization.

## Design Overview

Assumptions are made that GFS are on inexpensive machines running Linux. And the major optimization should be on files 100MB or larger in size as well multi-GB files while smaller files should be supported. The read operations maybe large sequential reads and small random reads, and both should be supported. The files are often used in producer-consumer queues for many-way merging, so atomicity with minial synchronization is critical. High bandwidth is more important than latency.

GFS provides a familiar file system interface for file manipulation and also supports snapshot and record append operations.

A GFS has a single master and multiple chunk servers and is concurrently visited by mutliple clients. Files are segmented into equal-sized chunks(typically 64MB) and each uniquely identified by a 64 bit chunk handle assigned by master at creation. Three replicas of these chunks are stored across chunk servers. The master is in charge of chunk lease management, garbage collection and chunk migration between chunk servers. 

When a client needs data, it sends the file name and offset(chunk index) to the master and the master returns the corresponding chunk size and chunk location in GFS and the client goes to the given chunkservers(most likely the closest one) for data.

One issue arises, due to large chunk size, that when a file is small, it only has one chunk. If multiple clients access it, it becomes hot spot, and I/O may become the bottleneck. One simple solution is to have a higher replication factor so that it can be accessed from many more machines which should leverage the load on only several machines. A P2P approach may also help solve this problem.

The master has the file and chunk namespaces, the mapping from files to chunks, and the locations of each chunks replicas. The chunk locations information is volatile and is acquired from chunk servers periodically after the master startup.

Operation logs are kept for recovery. 

GFS has a relaxed consistency model. GFS guarantees that file namespace mutations like file creations are atomic. GFS achieves this by applying mutations to a chunk in the smae order on all its replicas and using chunk version numbers to detect stale replicas.