# The Google File System (Page 6 - 10)

## System Interactions

Upon mutation operations(primarily appending to the end of page), the master chooses a primary chunkserver and assigns a lease(timestamp that typically expire in 60 seconds) to it and then the client communicates directly to the primary chunkserver. The primary picks the order in which all the concurrent mutations to it. Then the mutations to the primary are synchronized with the secondary chunks(replicas other than the primary) in a pipelined fashion, each chunkserver relays its newly written data to the nearest not updated one. A chunk server in the middle starts upload immediately it receives data from the previous node, so the time to synchronize is B/T + RL, with B referring to the total number of bytes, T being the network throughput, R being the number of replicas and L being the latency between two machines(a simplified latency model). When all the operations are finished or in case any of these fail, the primary will report it to the client. If a failure happens, the client should handle the error and retry.

Mutations are atomic by using a producer-comsumer queue, and starts from the primary chunkserver to append, and as mentioned, to other nodes one by one. The atomicity is made possible because GFS only appends to the end of a chunk. And if the chunk is not large enough for the new coming data, the chunkserver will pad to the chunk and return failure so that the client can retry to create a new chunk and append. If the write fails for other reasons, the client will also retry. Since there might be failures, and GFS only guarantees integrity of each atomic unit excluding the paddings betweens them, the replica chunks are not guaranteed to be the same.

GFS uses copy-on-write techniques to take snapshots. When the master receives a snapshot request, it revokes all the leases from chunkservers, making sure the client cache will fail and all the operations of the chunk goes to the master. It makes lazy replication possible, which greatly improves efficiency. The newly created snapshot files point to the same chunks as the source files. When a write operation is going to be performed on one of the cnunks, the reference count of the chunk is larger than one, the master will create a replica of the target chunk on the same chunk server and assign a lease to the new one and return it to the client.

## Master Operation
GFS uses prefix compression(trie) to store the hierarchy of directories and files and uses locks to ensure proper serilization. There two types of locks, one for read, and one for write. When there is mutation along the trie, all the nodes on the path are read-locked, this prevents the directories being deleted, renamed or snapshotted, while write lock prevents multiple writes simultaneously.

Replica placement has to maximzie the data reliability and network bandwidth utilization. The total bandwidth inside a rack may be larger than the bandwidth between racks, so having replicas among racks is important. Meanwhile, this causes write traffic flow through multiple racks, which is a tradeoff.

Chunk replicas are created for three reasons: chunk creation, re-replication, and rebalancing. When creating replica, we take the disk space utilization and most recent replica time on a chunkserver into consideration, and spread replicas of the same chunk to racks so that later I/O operations can be distributed as evenly as possible to chunkservers and racks.

As for garbage collection, the master logs the deletion and performs lazy and delayted deletion. The file chunks failed to create are reclaimed properly and the garbage collector works only when the compute resource usage is not that high.For stale replicas, the master maitains a chunk version number to distinguish up-to-date and stale replicas. The master removes stale replicas in its regular garbage collection.

## Fault Tolerance and Diagnosis
GFS has high availability. Both the master and the chunk server are designed to restore in seconds and all the chunks are replicated in different levels according to the requirements, so component failure won't affect the system much. When the master node is down, "shadow" masters will provide read-only access to the file system, but not write, since the "shadow" may lag the master in some degree. A shadow server will poll chunkservers at startup to locate chunk replicas, but it only depends on the primary master to create and delete replicas.

To guarantee data integrity, a chunk is broken up into 64KB blocks, each with a corresponding 32 bit checksum(totally 32bit*1000 = 4KB). A chunkserver verifies data integrity before handling any request, so as not to propagate any data fault. The checksum data is appended to the end of the chunks. The data integrity of the first and last block of the range being overwritten are also checked in advance to avoid hidden corruption.

For diagnosis purposes, logs and are kept for significant events like RPC and server down and up. And the logging is kept as much as possible when disk space allows.