## TiDB 存储

> https://pingcap.com/zh/blog/tidb-internal-1

### 数据存储系统的问题

- 保存数据不丢不错
- 能否支持跨数据中心的容灾？
- 写入速度是否够快？
- 数据保存下来后，是否方便读取？
- 保存的数据如何修改？如何支持并发的修改？
- 如何原子地修改多条记录？

### Key-Value

TiKV数据库

- 存储键值对，可以理解为Map
- 按Kry的二进制顺序有序
- 存储模型与SQL中的Table无关

### RocksDB

TiKV将数据保存到RocksDB中，具体的数据落地交给RocksDB实现

RocksDB：单机存储引擎，可以理解为一个单机的K-V Map

### Raft

使用Raft一致性算法，保证单机失效的情况下，数据不丢失

Raft提供的重要功能包括：Leader选举、成员变更、日志复制

TiKV利用Raft做数据复制，每个数据变更都是一条Raft日志，由Raft算法将日志同步到节点

### Region

一个KV系统将数据分散的两种方案：

1. 按Key做Hash，根据Hash值选择节点
2. 分Range，将一段连续的Key存储在同一个节点

**以Region为单位，将数据分散到所有节点，尽量保证每个节点上的Region数量相近**

- 实现了存储容量的水平扩展：增加新的节点后，自动调度其他节点上的Region 
- 实现了负载均衡：所有节点上Region数量相近
- 另一个组件，负责Key->Region->节点的查询

**以Region为单位做Raft的复制和成员管理**

- 一个Region保存多个副本，每个副本成为一个Replica
- Replica通过Raft保持一致

![](https://img1.www.pingcap.com/prod/4_7d840f500e.png)

### MVCC

目标场景：两个Client同时修改一个Key的Value，没有MVCC则需要加锁

TiKV的MVCC通过在Key上加Version实现

```
Key1-Version3 -> Value
Key1-Version2 -> Value
Key1-Version1 -> Value
……
Key2-Version4 -> Value
Key2-Version3 -> Value
Key2-Version2 -> Value
Key2-Version1 -> Value
……
KeyN-Version2 -> Value
KeyN-Version1 -> Value
```



### 事务

 [Percolator ](https://www.usenix.org/legacy/event/osdi10/tech/full_papers/Peng.pdf)模型

TiKV采用乐观锁，事务执行过程不检测写写冲突，只有在提交过程中做冲突检测。快的能写入，慢的尝试重新执行。

性能：在写入冲突少时性能好。由于造成了大量的无效重试，冲突多时性能差



