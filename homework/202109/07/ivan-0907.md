## 关系模型映射 KV 模型

> https://pingcap.com/zh/blog/tidb-internal-2

问题：在KV结构上保存Table、在KV结构上运行SQL语句

### 需求

#### 数据特点

Table存储的数据：表的元信息、Table中的Row，索引数据

元信息：暂不考虑

Row：由于面向OLTP 业务，需要支持快速地读取、保存、修改、删除一行数据，所以采用行存。

Index

- TiDB需要支持Primary Index和Secondary Index
- 需要支持点查和范围查询
- 需要支持Unique Index 和 非 Unique Index的查询

#### 操作需求

Insert：将Row写入KV，同时建立索引数据

Update：将Row更新，同时建立索引数据

Delete：删除Row，同时删除索引

Select：每个Row需要一个ID，能快速读取；当用索引查询时，支持点查与范围查询

### 能力

一个全局有序的分布式 Key-Value 引擎：TiKV能通过Key定位到一行数据所在的位置。

### 数据编码

TiDB 对每个表分配一个 TableID，每一个索引都会分配一个 IndexID，每一行分配一个 RowID。TableID 在整个集群内唯一，IndexID/RowID 在表内唯一

前缀：一个 Table 内部所有的 Row 都有相同的前缀。因此在TiKV的Key空间中，这些相同前缀的数据排列在一起。

后缀：设计一定的后缀方案，就可实现编码前和编码后的比较关系不变，这种实现称为Memcomparable。具体的编码方案参见 TiDB 的 [codec 包 ](https://github.com/pingcap/tidb/tree/master/util/codec)。

每行数据编码：

`tablePrefix`/`recordPrefixSep`为指定的字符串常量

```
Key: tablePrefix{tableID}_recordPrefixSep{rowID}
Value: [col1, col2, col3, col4]
```

Unique Index数据编码：

```
Key: tablePrefix{tableID}_indexPrefixSep{indexID}_indexedColumnsValue
Value: rowID
```

非Unique Index数据通过以上编码并不能构造出唯一的 Key，因为可能多行的indexedColumnsValue值相同

非Unique Index数据编码：

```
Key: tablePrefix{tableID}_indexPrefixSep{indexID}_indexedColumnsValue_rowID
Value: null
```

### 实现效果

一个表的所有 Row 数据按 RowID 的顺序排列在 TiKV 的 Key 空间中，某一个 Index 的数据也会按照 Index 的 ColumnValue 顺序排列在 Key 空间内。

## 元信息管理

1. Database/Table的元信息同样存储在TiKV中，Key的唯一性通过其被分配的ID实现。
2. TiDB使用专门的KV存储当前Schema信息的版本，有一个后台线程在不断的检查 TiKV 上面存储的 Schema 版本是否发生变化，并且保证在一定时间内一定能够获取版本的变化

## SQL on KV 架构

![structure](https://img1.www.pingcap.com/prod/1_745fc87460.png)

TiDB Server：处理用户请求，执行 SQL 运算逻辑

### SQL运算

一个查询语句是如何操作底层存储的数据

朴素的方案：通过SQL到KV的映射方案，将 SQL 查询映射为对 KV 的查询，再通过 KV 接口获取对应的数据，最后执行计算

例子：`Select count(*) from user where name="TiDB";`

- 构造 Key Range：一个表中所有的 RowID 都在 `[0, MaxInt64)`这个范围内，那么我们用 0 和 MaxInt64 根据 Row 的 Key 编码规则，就能构造出一个 `[StartKey, EndKey)`的左闭右开区间
- 扫描 Key Range：根据上面构造出的 Key Range，读取 TiKV 中的数据
- 过滤数据：对于读到的每一行数据，计算 `name="TiDB"`这个表达式，如果为真，则向上返回这一行，否则丢弃这一行数据
- 计算 Count：对符合要求的每一行，累计到 Count 值上面

**缺陷**

1. 在第二步扫描数据的时候，每一行都要通过 KV 操作从 TiKV 中读取出来，至少有一次 RPC 开销，如果需要扫描的数据很多，那么这个开销会非常大
2. 并不是所有的行都有用，如果不满足条件，其实可以不读取出来
3. 符合要求的行的值并没有什么意义，实际上这里只需要有几行数据这个信息就行

### 分布式 SQL 运算

> [MPP and SMP in TiDB ](https://mp.weixin.qq.com/s?__biz=MzI3NDIxNTQyOQ==&mid=2247484187&idx=1&sn=90a7ce3e6db7946ef0b7609a64e3b423&chksm=eb162471dc61ad679fc359100e2f3a15d64dd458446241bff2169403642e60a95731c6716841&scene=4)这篇文章详细描述了 TiDB 是如何让 SQL 语句跑的更快

1. 将计算尽量靠近存储节点，以避免大量的 RPC 调用
2. 将 Filter 下推到存储节点进行计算，这样只需要返回有效的行，避免无意义的网络传输
3. 将聚合函数、GroupBy 也下推到存储节点，进行预聚合，每个节点只需要返回一个 Count 值即可，再由 tidb-server 将 Count 值 Sum 起来

数据逐层返回的示意图

![](https://img1.www.pingcap.com/prod/2_3ad3e55f16.png)



