## 210511 Day0002 Page 6-10 of [MapReduce](https://pdos.csail.mit.edu/6.824/papers/mapreduce.pdf)

In this part, backup tasks are mentioned, which is a mechanism used to alleviate the overall slow down caused by some slow nodes. When some of the fast nodes have finished, they will not rest but to do the same computation as the slow ones. As long as the task is finished by either one of the nodes, we can get the result of the map task. There is a significant acceleration with this optimization.

Some other refinements are also provided. The partitioning function(hash function) can be customized to optimally meet our needs. Ordering guarantees are fast for random access and sorting. A combiner function can be useful in some case, which can partially merge high-frequency keys for better space and time efficiency. A signal system is provided to skip the failed map tasks in the reduce phase. Local Execution deploys the cluster on a single machine and makes it easy for testing, debugging and profiling. Status Information and Counters are implemented for additional information.

In terms of performance, with 1800 machines, sort task over 1 terabyte of data finish in 960 seconds. And when tasks are killed intentionally, they restore really quickly and there's no obvious degradation.

MapReduce has a wild application in Google, including large-scale machine learning problems, clustering problems, extraction of web pages and large-scale graph computations.